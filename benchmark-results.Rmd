---
title: "RaptorJIT benchmark results"
output:
  html_document:
    toc: true
    theme: united
    fig_height: 8
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(dplyr)
library(ggplot2)
library(knitr)
d <- read.csv("bench.csv")
```

# Overview

```{r include=FALSE}
runs <- mean(summarize(group_by(d, benchmark, raptorjit), runs = n())$runs)
A <- head(filter(d, letter == "A"), 1)$raptorjit
```

This is an automatically generated report showing benchmark results based on `r round(runs)` runs (iterations) per benchmark for each branch.

# Graphs

## Average relative performance of branches

Performance (average of `r round(runs)` runs) relative to baseline branch ``r A``.

```{r echo=FALSE, fig.width=12}
baseline <- d %>% filter(letter=="A") %>% group_by(benchmark) %>% summarize(baseline = mean(cycles))
relative <- d %>%
  left_join(baseline, by="benchmark") %>%
  group_by(benchmark, raptorjit) %>% mutate(relativemean = first(baseline) / mean(cycles))

ggplot(aes(y=relativemean, x=raptorjit, fill=benchmark), data=relative) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(axis.text.x = element_text(angle=90)) +
  scale_y_continuous(breaks=seq(0, 3, 0.1), labels=scales::percent) +
  ylab("relative performance") +
  ggtitle("Comparative performance")
```

## Variation in performance of each branch

The relative standard deviation (RSD) is a measure of how consistent benchmark results are when compared with other runs from the same branch. If the RSD is 0% then every run of the benchmark on the given branch performed exactly the same.

```{r echo=FALSE, fig.width=12}
rsd <- group_by(d, benchmark, raptorjit) %>% mutate(rsd=sd(cycles)/mean(cycles)) %>% ungroup()
ggplot(aes(y=rsd, x=raptorjit, fill=benchmark), data=rsd) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_y_continuous(breaks=seq(0, 1, 0.02), labels=scales::percent) +
  theme(axis.text.x = element_text(angle=90)) +
  ylab("relative standard deviation (RSD)")
```

## Instructions per cycle

The number of instructions retired per cycle (IPC) provides some insight into how the CPU deals with the generated code. Low IPC may indicate hazards such as cache misses and branch mispredictions.

```{r echo=FALSE, fig.width=12}
ipc <- group_by(d, benchmark, raptorjit) %>% mutate(ipc=instructions/cycles) %>% ungroup()
ggplot(aes(y=ipc, x=raptorjit, fill=benchmark), data=ipc) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(axis.text.x = element_text(angle=90)) +
  ylab("instructions/cycle") +
  ggtitle("CPU instructions retired per cycle")
```

# Numeric

## Summary statistics

Numeric summary statistics can be useful for extra precision and comparison between different reports.

```{r kable, echo=FALSE}
kable(
  col.names = c("Benchmark", "RaptorJIT branch", "Runs", "Mean running time (Gigacycles)", "Relative Standard Deviation (%)"),
  summarize(group_by(d, benchmark, raptorjit),
            runs = n(),
            mean = mean(cycles)/1000000000,
            rsd = round(sd(cycles)/1000000000*100/mean, 2)))
```
